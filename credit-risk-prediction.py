# -*- coding: utf-8 -*-
"""Credit Risk.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wvn6vF2-pcKwnW2xPMyA4Z55-wjBksiW

# CREDIT RISK PREDICTION

# Introduction

Credit Risk Prediction adalah suatu metode efektif untuk mengevaluasi potensi seorang calon peminjam dalam melunasi pinjaman yang diberikan. Dataset yang digunakan berasal dari LendingClub, sebuah perusahaan Peer-to-Peer (P2P) Lending yang berbasis di Amerika Serikat.

Dengan memanfaatkan data historis pinjaman yang diberikan oleh LendingClub, termasuk informasi mengenai keberhasilan atau kegagalan peminjam dalam membayar pinjaman, kita dapat mengembangkan model prediksi yang mampu memproyeksikan kemungkinan seorang peminjam untuk melunasi pinjaman di masa depan. Dengan menggunakan pendekatan ini, ketika kita menghadapi calon pelanggan baru di waktu yang akan datang, kita dapat melakukan penilaian terhadap kemampuan mereka dalam membayar pinjaman tersebut.

# Import Library
"""

import numpy as np

import pandas as pd
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 99)

import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

import warnings
warnings.filterwarnings("ignore")

from google.colab import drive
drive.mount('/content/drive')

"""# Import Dataset"""

data_raw = pd.read_csv('/content/drive/MyDrive/Dataset/Credit/loan_data_2007_2014.csv', index_col=0)

"""# Explore Dataset"""

data_raw.shape

data_raw.info()

data_raw.sample()

data_raw.id.nunique()

data_raw.member_id.nunique()

"""Terlihat bahwa tidak ada `id` atau `member_id` yang duplikat, artinya setiap baris sudah mewakili satu individu.

Selanjutnya, pembuangan fitur-fitur yang tidak berguna dilakukan. Contohnya seperti fitur yang merupakan id unik, berupa free text, nilainya kosong semua (NULL), dsb.
"""

cols_to_drop = [
    # unique id
    'id'
    , 'member_id'

    # free text
    , 'url'
    , 'desc'

    # all null / constant / others
    , 'zip_code'
    , 'annual_inc_joint'
    , 'dti_joint'
    , 'verification_status_joint'
    , 'open_acc_6m'
    , 'open_il_6m'
    , 'open_il_12m'
    , 'open_il_24m'
    , 'mths_since_rcnt_il'
    , 'total_bal_il'
    , 'il_util'
    , 'open_rv_12m'
    , 'open_rv_24m'
    , 'max_bal_bc'
    , 'all_util'
    , 'inq_fi'
    , 'total_cu_tl'
    , 'inq_last_12m'

    # expert judgment
    , 'sub_grade'
]

data = data_raw.drop(cols_to_drop, axis=1)

data.sample(5)

"""# Define Target Variable

Dalam project credit risk modeling, tujuan utama adalah untuk melakukan prediksi terhadap suatu individu akan kemampuan mereka untuk melakukan pembayaran terhadap pinjaman/kredit yang diberikan. Oleh karena itu, variabel target yang digunakan harus mencerminkan kemampuan individu dalam hal tersebut.

Dalam dataset ini, variabel `loan_status` adalah variabel yang dapat dijadikan variabel target karena mencerminkan performa masing-masing individu dalam melakukan pembayaran terhadap pinjaman/kredit selama ini.
"""

data.loan_status.value_counts(normalize=True)*100

"""Dapat dilihat bahwa variabel `loan_status` memiliki beberapa nilai:

* `Current` artinya pembayaran lancar
* `Charged Off` artinya pembayaran macet sehingga dihapusbukukan
* `Late` artinya pembayaran telat dilakukan
* `In Grace Period` artinya dalam masa tenggang; `Fully Paid` artinya pembayaran lunas
* `Default` artinya pembayaran macet

Dari definisi-definisi tersebut, masing-masing individu dapat ditandai apakah mereka merupakan `bad loan` (peminjam yang buruk) atau `good loan` (peminjam yang baik)

Definisi bad dan good loan terkadang bisa berbeda tergantung dari kebutuhan bisnis. Pada contoh ini, saya menggunakan keterlambatan pembayaran di atas 30 hari dan yang lebih buruk dari itu sebagai penanda bad loan.
"""

bad_status = [
    'Charged Off'
    , 'Default'
    , 'Does not meet the credit policy. Status:Charged Off'
    , 'Late (31-120 days)'
]

data['bad_flag'] = np.where(data['loan_status'].isin(bad_status), 1, 0)

data['bad_flag'].value_counts(normalize=True)*100

"""Setelah melakukan flagging terhadap bad/good loan, dapat dilihat bahwa jumlah individu yang ditandai sebagai bad loan jauh lebih sedikit daripada good loan. Hal ini menyebabkan problem ini menjadi problem imbalanced dataset.

Jangan lupa untuk membuang kolom asal `loan_status`
"""

data.drop('loan_status', axis=1, inplace=True)

"""# Cleaning, Preprocessing, Feature Engineering

Pada step ini, dilakukan pembersihan/modifikasi beberapa fitur ke dalam format yang dapat digunakan untuk modeling.

### emp_length

Memodifikasi `emp_length`. Contoh: 4 years -> 4
"""

data['emp_length'].unique()

data['emp_length_int'] = data['emp_length'].str.replace('\+ years', '')
data['emp_length_int'] = data['emp_length_int'].str.replace('< 1 year', str(0))
data['emp_length_int'] = data['emp_length_int'].str.replace(' years', '')
data['emp_length_int'] = data['emp_length_int'].str.replace(' year', '')

data['emp_length_int'] = data['emp_length_int'].astype(float)

data.drop('emp_length', axis=1, inplace=True)

"""### term

Memodifikasi `term`. Contoh: 36 months -> 36
"""

data['term'].unique()

data['term_int'] = data['term'].str.replace(' months', '')
data['term_int'] = data['term_int'].astype(float)

data.drop('term', axis=1, inplace=True)

"""### earliest_cr_line

Memodifikasi `earliest_cr_line` dari format bulan-tahun menjadi perhitungan berapa lama waktu berlalu sejak waktu tersebut. Untuk melakukan hal ini, umumnya digunakan reference date = hari ini. Namun, karena dataset ini merupakan dataset tahun 2007-2014, maka akan lebih relevan jika menggunakan reference date di sekitar tahun 2017. Dalam contoh ini, saya menggunakan tanggal 2017-12-01 sebagai reference date.
"""

data['earliest_cr_line'].head(3)

data['earliest_cr_line_date'] = pd.to_datetime(data['earliest_cr_line'], format='%b-%y')
data['earliest_cr_line_date'].head(3)

data['mths_since_earliest_cr_line'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - data['earliest_cr_line_date']) / np.timedelta64(1, 'M')))
data['mths_since_earliest_cr_line'].head(3)

data['mths_since_earliest_cr_line'].describe()

"""Terlihat ada nilai yang aneh, yaitu negatif."""

data[data['mths_since_earliest_cr_line']<0][['earliest_cr_line', 'earliest_cr_line_date', 'mths_since_earliest_cr_line']].head(3)

"""Ternyata nilai negatif muncul karena fungsi Python salah menginterpretasikan tahun 62 menjadi tahun 2062, padahal seharusnya merupakan tahun 1962.

Untuk mengatasi hal ini, kita akan mengubah `earliest_cr_line_date` tersebut menjadi string dan mengganti angka tahun 20 menjadi 19 pada data `mths_since_earliest_cr_line` yang masih negatif.
"""

data['earliest_cr_line_date'] = data['earliest_cr_line_date'].astype(str)
data['earliest_cr_line_date'][data['mths_since_earliest_cr_line'] < 0] = data['earliest_cr_line_date'][data['mths_since_earliest_cr_line'] < 0].str.replace('20','19')
data['earliest_cr_line_date'] = pd.to_datetime(data['earliest_cr_line_date'])

data[data['mths_since_earliest_cr_line']<0][['earliest_cr_line', 'earliest_cr_line_date', 'mths_since_earliest_cr_line']].head(3)

data['mths_since_earliest_cr_line'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - data['earliest_cr_line_date']) / np.timedelta64(1, 'M')))
data['mths_since_earliest_cr_line'].head(3)

data['mths_since_earliest_cr_line'].describe()

data.drop(['earliest_cr_line', 'earliest_cr_line_date'], axis=1, inplace=True)

"""### issue_d

Konsep preprocessing yang dilakukan sama dengan yang dilakukan terhadap variabel `earliest_cr_line`
"""

data['issue_d_date'] = pd.to_datetime(data['issue_d'], format='%b-%y')
data['mths_since_issue_d'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - data['issue_d_date']) / np.timedelta64(1, 'M')))

data['mths_since_issue_d'].describe()

data.drop(['issue_d', 'issue_d_date'], axis=1, inplace=True)

"""### last_pymnt_d

Konsep preprocessing yang dilakukan sama dengan yang dilakukan terhadap variabel `earliest_cr_line`
"""

data['last_pymnt_d_date'] = pd.to_datetime(data['last_pymnt_d'], format='%b-%y')
data['mths_since_last_pymnt_d'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - data['last_pymnt_d_date']) / np.timedelta64(1, 'M')))

data['mths_since_last_pymnt_d'].describe()

data.drop(['last_pymnt_d', 'last_pymnt_d_date'], axis=1, inplace=True)

"""### next_pymnt_d

Konsep preprocessing yang dilakukan sama dengan yang dilakukan terhadap variabel `earliest_cr_line`
"""

data['next_pymnt_d_date'] = pd.to_datetime(data['next_pymnt_d'], format='%b-%y')
data['mths_since_next_pymnt_d'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - data['next_pymnt_d_date']) / np.timedelta64(1, 'M')))

data['mths_since_next_pymnt_d'].describe()

data.drop(['next_pymnt_d', 'next_pymnt_d_date'], axis=1, inplace=True)

"""### last_credit_pull_d

Konsep preprocessing yang dilakukan sama dengan yang dilakukan terhadap variabel `earliest_cr_line`
"""

data['last_credit_pull_d_date'] = pd.to_datetime(data['last_credit_pull_d'], format='%b-%y')
data['mths_since_last_credit_pull_d'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - data['last_credit_pull_d_date']) / np.timedelta64(1, 'M')))

data['mths_since_last_credit_pull_d'].describe()

data.drop(['last_credit_pull_d', 'last_credit_pull_d_date'], axis=1, inplace=True)

"""# Exploratory Data Analysis

### Correlation Check
"""

plt.figure(figsize=(20,20))
sns.heatmap(data.corr())

"""Di sini, jika ada pasangan fitur-fitur yang memiliki korelasi tinggi maka akan diambil salah satu saja. Nilai korelasi yang dijadikan patokan sebagai korelasi tinggi tidak pasti, umumnya digunakan angka 0.7."""

corr_matrix = data.corr().abs()
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape, dtype=np.bool_), k=1))
to_drop_hicorr = [column for column in upper.columns if any(upper[column] > 0.7)]

to_drop_hicorr

data.drop(to_drop_hicorr, axis=1, inplace=True)

"""### Check Categorical Features"""

data.select_dtypes(include='object').nunique()

"""Pada tahap ini dilakukan pembuangan fitur yang memiliki nilai unik yang sangat tinggi (high cardinality) dan fitur yang hanya memiliki satu nilai unik saja."""

data.drop(['emp_title', 'title', 'application_type'], axis=1, inplace=True)

data.select_dtypes(exclude='object').nunique()

"""Ternyata, pada tipe data selain `object` juga terdapat fitur yang hanya memiliki satu nilai unik saja, maka akan ikut dibuang juga."""

data.drop(['policy_code'], axis=1, inplace=True)

for col in data.select_dtypes(include='object').columns.tolist():
    print(data[col].value_counts(normalize=True)*100)
    print('\n')

"""Fitur yang sangat didominasi oleh salah satu nilai saja akan dibuang pada tahap ini."""

data.drop('pymnt_plan', axis=1, inplace=True)

"""# Missing Values

### Missing Value Checking
"""

check_missing = data.isnull().sum() * 100 / data.shape[0]
check_missing[check_missing > 0].sort_values(ascending=False)

"""Di sini, kolom-kolom dengan missing values di atas 75% dibuang"""

data.drop('mths_since_last_record', axis=1, inplace=True)

"""### Missing Values Filling"""

data['annual_inc'].fillna(data['annual_inc'].mean(), inplace=True)
data['mths_since_earliest_cr_line'].fillna(0, inplace=True)
data['acc_now_delinq'].fillna(0, inplace=True)
data['total_acc'].fillna(0, inplace=True)
data['pub_rec'].fillna(0, inplace=True)
data['open_acc'].fillna(0, inplace=True)
data['inq_last_6mths'].fillna(0, inplace=True)
data['delinq_2yrs'].fillna(0, inplace=True)
data['collections_12_mths_ex_med'].fillna(0, inplace=True)
data['revol_util'].fillna(0, inplace=True)
data['emp_length_int'].fillna(0, inplace=True)
data['tot_cur_bal'].fillna(0, inplace=True)
data['tot_coll_amt'].fillna(0, inplace=True)
data['mths_since_last_delinq'].fillna(-1, inplace=True)

"""# Feature Scaling and Transformation

### One Hot Encoding

Semua kolom kategorikal dilakukan One Hot Encoding.
"""

categorical_cols = [col for col in data.select_dtypes(include='object').columns.tolist()]

onehot = pd.get_dummies(data[categorical_cols], drop_first=True)

onehot.head()

"""### Standardization

Semua kolom numerikal dilakukan proses standarisasi dengan StandardScaler.
"""

numerical_cols = [col for col in data.columns.tolist() if col not in categorical_cols + ['bad_flag']]

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
std = pd.DataFrame(ss.fit_transform(data[numerical_cols]), columns=numerical_cols)

std.head()

"""### Transformed Dataframe

Menggabungkan kembali kolom-kolom hasil transformasi
"""

data_model = pd.concat([onehot, std, data[['bad_flag']]], axis=1)

"""# MODELING

### Train-Test Split
"""

from sklearn.model_selection import train_test_split

X = data_model.drop('bad_flag', axis=1)
y = data_model['bad_flag']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.shape, X_test.shape

"""### Training

Kita akan menggunakan algoritma XGBoost dan Random Forest untuk pemodelan.
"""

from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier

# XGBoost
xgb = XGBClassifier(learning_rate=0.1, max_depth=10, n_estimators=100, gamma=0, subsample=0.8, colsample_bytree=0.8)
xgb.fit(X_train, y_train)

# Random Forest
rf = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=2, min_samples_leaf=1, max_features='auto')
rf.fit(X_train, y_train)

"""Feature Importance dapat ditampilkan."""

# Feature Importance XGBoost
arr_feature_importances_xgb = xgb.feature_importances_
arr_feature_names = X_train.columns.values

df_feature_importance_xgb = pd.DataFrame(index=range(len(arr_feature_importances_xgb)), columns=['feature', 'importance'])
df_feature_importance_xgb['feature'] = arr_feature_names
df_feature_importance_xgb['importance'] = arr_feature_importances_xgb
df_all_features_xgb = df_feature_importance_xgb.sort_values(by='importance', ascending=False)
df_all_features_xgb

# Feature Importance Random Forest
arr_feature_importances_rf = rf.feature_importances_
arr_feature_names = X_train.columns.values

df_feature_importance_rf = pd.DataFrame(index=range(len(arr_feature_importances_rf)), columns=['feature', 'importance'])
df_feature_importance_rf['feature'] = arr_feature_names
df_feature_importance_rf['importance'] = arr_feature_importances_rf
df_all_features_rf = df_feature_importance_rf.sort_values(by='importance', ascending=False)
df_all_features_rf

"""### Validation

Untuk mengukur performa model, dua metrik yang umum dipakai dalam dunia credit risk adalah AUC dan KS. Namun, saya juga ingin melihat metrik lain seperti Akurasi, F1-score, Recall dll dari model yang dibuat menggunakan classification report.
"""

# XGBoost
y_pred_proba_xgb = xgb.predict_proba(X_test)[:][:,1]
df_actual_predicted_xgb = pd.concat([pd.DataFrame(np.array(y_test), columns=['y_actual']), pd.DataFrame(y_pred_proba_xgb, columns=['y_pred_proba_xgb'])], axis=1)
df_actual_predicted_xgb.index = y_test.index

# Random Forest
y_pred_proba_rf = rf.predict_proba(X_test)[:][:,1]
df_actual_predicted_rf = pd.concat([pd.DataFrame(np.array(y_test), columns=['y_actual']), pd.DataFrame(y_pred_proba_rf, columns=['y_pred_proba_rf'])], axis=1)
df_actual_predicted_rf.index = y_test.index

"""#### AUC"""

from sklearn.metrics import roc_curve, roc_auc_score

# AUC XGBoost
fpr, tpr, tr = roc_curve(df_actual_predicted_xgb['y_actual'], df_actual_predicted_xgb['y_pred_proba_xgb'])
auc = roc_auc_score(df_actual_predicted_xgb['y_actual'], df_actual_predicted_xgb['y_pred_proba_xgb'])

plt.plot(fpr, tpr, label='AUC = %0.4f' %auc)
plt.plot(fpr, fpr, linestyle = '--', color='k')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve XGBoost')
plt.legend()

# AUC Random Forest
fpr, tpr, tr = roc_curve(df_actual_predicted_rf['y_actual'], df_actual_predicted_rf['y_pred_proba_rf'])
auc = roc_auc_score(df_actual_predicted_rf['y_actual'], df_actual_predicted_rf['y_pred_proba_rf'])

plt.plot(fpr, tpr, label='AUC = %0.4f' %auc)
plt.plot(fpr, fpr, linestyle = '--', color='k')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Random Forest')
plt.legend()

"""#### KS"""

# KS XGBoost
df_actual_predicted_xgb = df_actual_predicted_xgb.sort_values('y_pred_proba_xgb')
df_actual_predicted_xgb = df_actual_predicted_xgb.reset_index()

df_actual_predicted_xgb['Cumulative N Population'] = df_actual_predicted_xgb.index + 1
df_actual_predicted_xgb['Cumulative N Bad'] = df_actual_predicted_xgb['y_actual'].cumsum()
df_actual_predicted_xgb['Cumulative N Good'] = df_actual_predicted_xgb['Cumulative N Population'] - df_actual_predicted_xgb['Cumulative N Bad']
df_actual_predicted_xgb['Cumulative Perc Population'] = df_actual_predicted_xgb['Cumulative N Population'] / df_actual_predicted_xgb.shape[0]
df_actual_predicted_xgb['Cumulative Perc Bad'] = df_actual_predicted_xgb['Cumulative N Bad'] / df_actual_predicted_xgb['y_actual'].sum()
df_actual_predicted_xgb['Cumulative Perc Good'] = df_actual_predicted_xgb['Cumulative N Good'] / (df_actual_predicted_xgb.shape[0] - df_actual_predicted_xgb['y_actual'].sum())

df_actual_predicted_xgb.head()

KS_xgb = max(df_actual_predicted_xgb['Cumulative Perc Good'] - df_actual_predicted_xgb['Cumulative Perc Bad'])

plt.plot(df_actual_predicted_xgb['y_pred_proba_xgb'], df_actual_predicted_xgb['Cumulative Perc Bad'], color='r')
plt.plot(df_actual_predicted_xgb['y_pred_proba_xgb'], df_actual_predicted_xgb['Cumulative Perc Good'], color='b')
plt.xlabel('Estimated Probability for Being Bad')
plt.ylabel('Cumulative %')
plt.title('Kolmogorov-Smirnov XGBoost:  %0.4f' %KS_xgb)

"""Model XGBoost yang dibangun menghasilkan performa `AUC = 0.910` dan `KS = 0.646`. Pada dunia credit risk modeling, umumnya AUC di atas 0.7 dan KS di atas 0.3 sudah termasuk performa yang baik."""

# KS Random Forest
df_actual_predicted_rf = df_actual_predicted_rf.sort_values('y_pred_proba_rf')
df_actual_predicted_rf = df_actual_predicted_rf.reset_index()

df_actual_predicted_rf['Cumulative N Population'] = df_actual_predicted_rf.index + 1
df_actual_predicted_rf['Cumulative N Bad'] = df_actual_predicted_rf['y_actual'].cumsum()
df_actual_predicted_rf['Cumulative N Good'] = df_actual_predicted_rf['Cumulative N Population'] - df_actual_predicted_rf['Cumulative N Bad']
df_actual_predicted_rf['Cumulative Perc Population'] = df_actual_predicted_rf['Cumulative N Population'] / df_actual_predicted_rf.shape[0]
df_actual_predicted_rf['Cumulative Perc Bad'] = df_actual_predicted_rf['Cumulative N Bad'] / df_actual_predicted_rf['y_actual'].sum()
df_actual_predicted_rf['Cumulative Perc Good'] = df_actual_predicted_rf['Cumulative N Good'] / (df_actual_predicted_rf.shape[0] - df_actual_predicted_rf['y_actual'].sum())

df_actual_predicted_rf.head()

KS_rf = max(df_actual_predicted_rf['Cumulative Perc Good'] - df_actual_predicted_rf['Cumulative Perc Bad'])

plt.plot(df_actual_predicted_rf['y_pred_proba_rf'], df_actual_predicted_rf['Cumulative Perc Bad'], color='r')
plt.plot(df_actual_predicted_rf['y_pred_proba_rf'], df_actual_predicted_rf['Cumulative Perc Good'], color='b')
plt.xlabel('Estimated Probability for Being Bad')
plt.ylabel('Cumulative %')
plt.title('Kolmogorov-Smirnov Random Forest:  %0.4f' %KS_rf)

"""Model Random Forest yang dibangun menghasilkan performa `AUC = 0.872` dan `KS = 0.586`. Pada dunia credit risk modeling, umumnya AUC di atas 0.7 dan KS di atas 0.3 sudah termasuk performa yang baik. Meski demikian peforma model XGBoost lebih baik jika dibandingkan dengan model Random Forest.

### Classification Report
"""

from sklearn.metrics import classification_report

# XGBoost
y_pred_xgb = xgb.predict(X_test)

print('Classification_Report:')
print(classification_report(y_test, y_pred_xgb, digits=4))

"""
Model XGBoost yang telah dibangun menunjukkan tingkat akurasi yang baik sebesar 0.944. Namun, terdapat perbedaan yang signifikan pada nilai recall antara kelas "bad loan" yang hanya mencapai 0.51, sementara kelas "good loan" mencapai 0.99. Indikasi ini menunjukkan kemungkinan terjadinya overfitting pada model. Salah satu faktor yang dapat menyebabkan perbedaan ini adalah ketidakseimbangan data dalam dataset."""

# Random Forest
y_pred_rf = rf.predict(X_test)

print('Classification_Report:')
print(classification_report(y_test, y_pred_rf, digits=4))

"""Model Random Forest yang telah dibangun menunjukkan tingkat akurasi yang lebih rendah jika dibandingkan dengan model XGBoost yaitu sebesar 0.942.

# Rekomendasi

Beberapa hal lain yang dapat dilakukan untuk project ini:

- Jika menginginkan interpretabilitas yang lebih tinggi, dapat mempertimbangkan untuk membuat Credit Scorecard dengan menggunakan algoritma Logistic Regression dengan pendekatan-pendekatannya seperti Feature Selection menggunakan Information Value dan Feature Engineering menggunakan Weight of Evidence.

- Melakukan hyperparameter tuning.

- Melakukan pemeriksaan atau memastikan bahwa model yang telah dibuat tidak overfitting. Hal ini dapat dilakukan dengan mencoba membandingkan hasil performa model ketika diprediksi terhadap data training dan ketika diprediksi terhadap data testing.

- Menangani ketidakseimbangan data dengan melakukan oversampling atau undersampling
"""